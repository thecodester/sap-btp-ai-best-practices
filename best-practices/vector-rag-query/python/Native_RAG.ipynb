{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with HANA Vector Store\n",
    "\n",
    "This notebook walks you through building a **Retrieval-Augmented Generation (RAG)** application using:\n",
    "- **HANA Vector Store**\n",
    "- **OpenAI GPT-4o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Dependencies\n",
    "Ensure the libraries mentinoed in the `requirements.txt` file are installed.\n",
    "\n",
    "\n",
    "### .env Variables\n",
    "Create a `.env` file with the following:\n",
    "```\n",
    "HANA_ADDRESS=your_hana_host\n",
    "HANA_PORT=your_port\n",
    "HANA_USER=your_user\n",
    "HANA_PASSWORD=your_password\n",
    "HANA_AUTOCOMMIT=true\n",
    "HANA_SSL_CERT_VALIDATE=false\n",
    "\n",
    "AICORE_AUTH_URL=your_aicore_auth_url\n",
    "AICORE_CLIENT_ID=your_aicore_client_id\n",
    "AICORE_CLIENT_SECRET=your_aicore_secret\n",
    "AICORE_RESOURCE_GROUP=your_resource_group\n",
    "AICORE_BASE_URL=your_base_url\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from hana_ml import ConnectionContext\n",
    "\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from gen_ai_hub.proxy.native.openai import embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Use the user's query to retrive most semantically similar documents from the vector store to create context that will be used to ground LLM for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00.000.00.1715685275 (fa/CE2024.2)\n",
      "USR_336RA2ZQ5LAGTHKHCKIYB945E\n"
     ]
    }
   ],
   "source": [
    "# Connect to SAP HANA\n",
    "cc = ConnectionContext(\n",
    "    address=os.environ.get(\"HANA_ADDRESS\"),\n",
    "    port=os.environ.get(\"HANA_PORT\"),\n",
    "    user=os.environ.get(\"HANA_USER\"),\n",
    "    password=os.environ.get(\"HANA_PASSWORD\"),\n",
    "    encrypt=True\n",
    ")\n",
    "\n",
    "cursor = cc.connection.cursor()\n",
    "\n",
    "print(cc.hana_version())\n",
    "print(cc.get_current_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AI Core proxy client\n",
    "proxy_client = get_proxy_client('gen-ai-hub')\n",
    "\n",
    "def get_embedding(query):\n",
    "    \"\"\"\n",
    "    Create embedding vector for given text.\n",
    "    \"\"\"\n",
    "    embeds = embeddings.create(\n",
    "        model_name=\"text-embedding-ada-002\",\n",
    "        input=query\n",
    "    )\n",
    "    return embeds.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vector_search(query, cursor, table_name, metric=\"COSINE_SIMILARITY\", k=4):\n",
    "    \"\"\"\n",
    "    Performs vector search on indexed documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_vector = get_embedding(query)\n",
    "        if not query_vector:\n",
    "            raise ValueError(\"Failed to generate query embedding.\")\n",
    "\n",
    "        sort_order = \"DESC\" if metric != \"L2DISTANCE\" else \"ASC\"\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT TOP {k} ID, MY_TEXT, MY_METADATA\n",
    "        FROM {table_name}\n",
    "        ORDER BY {metric}(MY_VECTOR, TO_REAL_VECTOR('{query_vector}')) {sort_order}\n",
    "        \"\"\"\n",
    "        cursor.execute(sql_query)\n",
    "        return cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during vector search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to test for fat in foods?\"\n",
    "\n",
    "# Retrieve top 4 matching docs from vector store\n",
    "context_records = run_vector_search(query, cursor, \"SCIENCE_DATA_MIT6\", 'COSINE_SIMILARITY', 4)\n",
    "# Join the content from retrieved docs\n",
    "context = ' '.join([c[1] for c in context_records])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment\n",
    "\n",
    "Augment the prompt instructions by embedding retrieved context into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Use the following context information to answer to user's query.\n",
    "Here is some context: {context}\n",
    "\n",
    "Based on the above context, answer the following query:\n",
    "{query}\n",
    "\n",
    "The answer tone has to be very proffesional in nature.\n",
    "\n",
    "If you don't know the answer, politely say that you don't know, don't try to make up an answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "Use an LLM from Generative AI Hub to generate response for the context augmented prompt. This allows the LLM to be grounded on the context while answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test for fats in food items, a practical method involves wrapping the food in a piece of paper and crushing it. Observing for an oily patch on the paper, especially when held against light, indicates the presence of fats. Additionally, rubbing the food item directly on the paper and allowing it to dry can also reveal fats by leaving an oily residue that remains visible after any water content has evaporated. This traditional technique provides a straightforward assessment of fat content in various foods.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o\", messages=messages)\n",
    "\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
